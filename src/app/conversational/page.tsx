"use client";
import React, { useState, useRef, useEffect } from 'react';
import { useRouter } from 'next/navigation';
import VoiceOrb from '@/components/VoiceOrb';

interface ConversationSession {
  sessionId: string | null;
  isActive: boolean;
  currentAudio: HTMLAudioElement | null;
}

interface InteractionResponse {
  success: boolean;
  aiResponse: string;
  audioResponse?: string;
  learningProgress?: {
    goalsMet: number;
    totalGoals: number;
    currentGoals: string[];
  };
}

export default function ConversationalAI() {
  const router = useRouter();

  // Auth check
  useEffect(() => {
    const isLoggedIn = localStorage.getItem('isLoggedIn');
    if (isLoggedIn !== 'true') {
      router.push('/');
    }
  }, [router]);

  const [session, setSession] = useState<ConversationSession>({
    sessionId: null,
    isActive: false,
    currentAudio: null,
  });

  const [conversationHistory, setConversationHistory] = useState<Array<{type: 'user' | 'ai', text: string}>>([]);
  const [learningProgress, setLearningProgress] = useState<any>(null);
  const [error, setError] = useState<string | null>(null);
  const [isProcessing, setIsProcessing] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isUserSpeaking, setIsUserSpeaking] = useState(false);
  const [persona, setPersona] = useState<'adult' | 'kid'>('adult');

  // Refs for audio processing
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const rafRef = useRef<number | null>(null);
  const isSpeakingRef = useRef(false);
  const speechStartTimeRef = useRef<number | null>(null);
  const sessionActiveRef = useRef(false);
  const sessionIdRef = useRef<string | null>(null);
  const audioUnlockedRef = useRef(false);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);

  const API_BASE = process.env.NEXT_PUBLIC_BACKEND_URL || 'http://localhost:8080';

  // Simple VAD thresholds - EXTREMELY SENSITIVE FOR NORMAL TALKING
  const SPEECH_THRESHOLD = 0.0150;  // Very low for normal conversation
  const SILENCE_THRESHOLD = 0.0070;
  const MIN_SPEECH_DURATION = 100;  // Start recording almost immediately
  const SILENCE_DURATION = 700;     // Stop recording after 700ms silence

  // Unlock audio playback (must be called from user gesture)
  function unlockAudio() {
    if (audioUnlockedRef.current) return;

    console.log('[Audio] Unlocking audio playback');

    // Create silent audio and play it to unlock
    const silentAudio = new Audio('data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU4Ljc2LjEwMAAAAAAAAAAAAAAA//tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAADhAC7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7//////////////////////////////////////////////////////////////////8AAAAATGF2YzU4LjEzAAAAAAAAAAAAAAAAJAAAAAAAAAAAA4SpmwWWAAAAAAAAAAAAAAAAAAAAAAD/+xDEAAPAAAGkAAAAIAAANIAAAARMQU1FMy4xMDBVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV');

    silentAudio.play()
      .then(() => {
        console.log('[Audio] Audio unlocked successfully');
        audioUnlockedRef.current = true;
      })
      .catch((err) => {
        console.warn('[Audio] Failed to unlock audio:', err);
      });
  }

  // Base64 to blob
  function base64ToBlob(base64: string, mimeType: string): Blob {
    const byteCharacters = atob(base64);
    const byteArrays = [];

    for (let offset = 0; offset < byteCharacters.length; offset += 512) {
      const slice = byteCharacters.slice(offset, offset + 512);
      const byteNumbers = new Array(slice.length);
      for (let i = 0; i < slice.length; i++) {
        byteNumbers[i] = slice.charCodeAt(i);
      }
      const byteArray = new Uint8Array(byteNumbers);
      byteArrays.push(byteArray);
    }

    return new Blob(byteArrays, { type: mimeType });
  }

  // Stop AI audio - AGGRESSIVELY
  function stopAIAudio() {
    console.log('[Audio] Interrupting AI speech');

    // Stop ref audio
    if (currentAudioRef.current) {
      try {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current.onended = null;
      } catch (e) {}
      currentAudioRef.current = null;
    }

    // Stop state audio (fallback)
    if (session.currentAudio) {
      try {
        session.currentAudio.pause();
        session.currentAudio.currentTime = 0;
        session.currentAudio.onended = null;
      } catch (e) {}
    }

    setIsSpeaking(false);
    setSession(prev => ({ ...prev, currentAudio: null }));
  }

  // Send audio to backend
  async function sendAudioToBackend(audioBlob: Blob) {
    if (!sessionIdRef.current) {
      console.error('[Audio] No sessionId available, cannot send audio');
      return;
    }

    try {
      setIsProcessing(true);
      setError(null);

      console.log('[Audio] Sending audio to backend, size:', audioBlob.size);

      const formData = new FormData();
      formData.append('sessionId', sessionIdRef.current);
      formData.append('audioChunk', audioBlob, 'audio.webm');

      const response = await fetch(`${API_BASE}/api/conversational-learning/stream/interact`, {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        throw new Error('Failed to process audio');
      }

      const data: InteractionResponse = await response.json();

      console.log('[Audio] Response received:', data);

      // Update conversation history
      setConversationHistory(prev => [
        ...prev,
        { type: 'user', text: '[Audio Message]' },
        { type: 'ai', text: data.aiResponse }
      ]);

      // Update learning progress
      if (data.learningProgress) {
        setLearningProgress(data.learningProgress);
      }

      // Play AI audio response
      if (data.success && data.audioResponse) {
        playAIAudio(data.audioResponse, data.aiResponse);
      }
    } catch (err) {
      console.error('[Audio] Error:', err);
      setError(err instanceof Error ? err.message : 'Failed to process audio');
    } finally {
      setIsProcessing(false);
    }
  }

  // Play AI audio response
  function playAIAudio(base64Audio: string, textResponse: string) {
    console.log('[Audio] Playing AI response');

    // CRITICAL: Stop any existing audio before playing new one
    stopAIAudio();

    const audioBlob = base64ToBlob(base64Audio, 'audio/mpeg');
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    // Register AI speaking
    if (sessionIdRef.current) {
      fetch(`${API_BASE}/api/conversational-learning/stream/ai-speaking`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
        body: new URLSearchParams({
          sessionId: sessionIdRef.current,
          message: textResponse,
          estimatedDurationMs: String(textResponse.length * 50),
        }),
      }).catch(() => {});
    }

    setIsSpeaking(true);
    currentAudioRef.current = audio;
    setSession(prev => ({ ...prev, currentAudio: audio }));

    // Play audio with error handling
    audio.play().catch((err) => {
      console.error('[Audio] Failed to play audio:', err);
      setError('Failed to play audio. Please ensure audio is allowed in your browser.');
      setIsSpeaking(false);
      currentAudioRef.current = null;
      setSession(prev => ({ ...prev, currentAudio: null }));
      URL.revokeObjectURL(audioUrl);
    });

    audio.onended = () => {
      console.log('[Audio] AI finished speaking');
      setIsSpeaking(false);
      currentAudioRef.current = null;
      if (sessionIdRef.current) {
        fetch(`${API_BASE}/api/conversational-learning/stream/ai-finished`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
          body: new URLSearchParams({ sessionId: sessionIdRef.current }),
        }).catch(() => {});
      }
      setSession(prev => ({ ...prev, currentAudio: null }));
      URL.revokeObjectURL(audioUrl);
    };
  }

  // Start recording
  function startRecording() {
    if (!mediaStreamRef.current || mediaRecorderRef.current?.state === 'recording') return;

    console.log('[VAD] Starting recording');
    audioChunksRef.current = [];

    const mediaRecorder = new MediaRecorder(mediaStreamRef.current, {
      mimeType: 'audio/webm'
    });

    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunksRef.current.push(event.data);
      }
    };

    mediaRecorder.onstop = () => {
      const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
      console.log('[VAD] Recording stopped, blob size:', audioBlob.size);
      if (audioBlob.size > 0) {
        sendAudioToBackend(audioBlob);
      }
    };

    mediaRecorderRef.current = mediaRecorder;
    mediaRecorder.start();
    setIsUserSpeaking(true);
  }

  // Stop recording
  function stopRecording() {
    if (mediaRecorderRef.current?.state === 'recording') {
      console.log('[VAD] Stopping recording');
      mediaRecorderRef.current.stop();
      setIsUserSpeaking(false);
    }
  }

  // Voice activity detection loop
  function startVADLoop() {
    if (!analyserRef.current) return;

    const analyser = analyserRef.current;
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    let lastSpeechTime = 0;
    let lastSilenceTime = Date.now();

    const detectVoice = () => {
      if (!sessionActiveRef.current) {
        console.log('[VAD] Loop stopped - session not active');
        return;
      }

      analyser.getByteTimeDomainData(dataArray);

      // Calculate RMS
      let sum = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const normalized = (dataArray[i] - 128) / 128;
        sum += normalized * normalized;
      }
      const rms = Math.sqrt(sum / dataArray.length);
      setAudioLevel(rms);

      const now = Date.now();

      // Debug: log RMS levels every 500ms
      if (Math.floor(now / 500) !== Math.floor(lastSilenceTime / 500)) {
        console.log('[VAD] RMS:', rms.toFixed(4), '| Recording:', isSpeakingRef.current, '| Threshold:', SPEECH_THRESHOLD);
      }

      // Speech detected
      if (rms > SPEECH_THRESHOLD) {
        lastSpeechTime = now;

        // Start recording if not already
        if (!isSpeakingRef.current) {
          if (speechStartTimeRef.current === null) {
            speechStartTimeRef.current = now;
          } else if (now - speechStartTimeRef.current > MIN_SPEECH_DURATION) {
            console.log('[VAD] Speech detected, starting recording');
            isSpeakingRef.current = true;
            stopAIAudio(); // Interrupt AI
            startRecording();
          }
        }
      }
      // Silence detected
      else if (rms < SILENCE_THRESHOLD) {
        lastSilenceTime = now;

        // Stop recording after silence duration
        if (isSpeakingRef.current && (now - lastSpeechTime > SILENCE_DURATION)) {
          console.log('[VAD] Silence detected, stopping recording');
          isSpeakingRef.current = false;
          speechStartTimeRef.current = null;
          stopRecording();
        }
      }

      rafRef.current = requestAnimationFrame(detectVoice);
    };

    detectVoice();
  }

  // Initialize audio system
  async function initializeAudio() {
    try {
      console.log('[Audio] Initializing audio system');

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      mediaStreamRef.current = stream;

      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      audioContextRef.current = audioContext;

      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      analyser.smoothingTimeConstant = 0.3;
      source.connect(analyser);
      analyserRef.current = analyser;

      console.log('[Audio] Audio system initialized');
      startVADLoop();
    } catch (err) {
      console.error('[Audio] Failed to initialize:', err);
      throw err;
    }
  }

  // Cleanup audio
  function cleanupAudio() {
    console.log('[Audio] Cleaning up');

    if (rafRef.current) {
      cancelAnimationFrame(rafRef.current);
      rafRef.current = null;
    }

    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop();
    }

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    analyserRef.current = null;
    isSpeakingRef.current = false;
    speechStartTimeRef.current = null;
  }

  // Start conversation
  async function startConversation() {
    try {
      setError(null);
      console.log('[Session] Starting conversation with persona:', persona);

      // CRITICAL: Unlock audio on user gesture
      unlockAudio();

      const response = await fetch(`${API_BASE}/api/conversational-learning/stream/start?persona=${persona}`, {
        method: 'POST',
      });

      if (!response.ok) {
        throw new Error('Failed to start session');
      }

      const data = await response.json();
      console.log('[Session] Session started:', data.sessionId);

      // Set refs FIRST
      sessionIdRef.current = data.sessionId;
      sessionActiveRef.current = true;

      setSession({
        sessionId: data.sessionId,
        isActive: true,
        currentAudio: null,
      });

      setConversationHistory([]);
      setLearningProgress(null);

      // Initialize audio
      await initializeAudio();
      console.log('[VAD] VAD loop should be running now');
    } catch (err) {
      console.error('[Session] Error starting:', err);
      setError(err instanceof Error ? err.message : 'Failed to start session');
    }
  }

  // End conversation
  async function endConversation() {
    const currentSessionId = sessionIdRef.current;
    if (!currentSessionId) return;

    try {
      console.log('[Session] Ending conversation');

      sessionActiveRef.current = false;
      cleanupAudio();
      stopAIAudio();

      await fetch(`${API_BASE}/api/conversational-learning/stream/complete?sessionId=${currentSessionId}`, {
        method: 'POST',
      });

      sessionIdRef.current = null;
      setSession({
        sessionId: null,
        isActive: false,
        currentAudio: null,
      });

      console.log('[Session] Conversation ended');
    } catch (err) {
      console.error('[Session] Error ending:', err);
      setError(err instanceof Error ? err.message : 'Failed to end session');
    }
  }

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      cleanupAudio();
      stopAIAudio();
    };
  }, []);

  const statusLabel = () => {
    if (!session.isActive) return 'Ready';
    if (isProcessing) return 'Processing';
    if (isSpeaking) return 'AI Speaking';
    if (isUserSpeaking) return 'You Speaking';
    return 'Listening';
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-purple-50 to-blue-50 dark:from-gray-900 dark:to-gray-800 p-6">
      <div className="max-w-7xl mx-auto">
        {/* Header */}
        <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6 mb-6">
          <h1 className="text-3xl font-bold text-gray-800 dark:text-white mb-1">
            Conversational AI
          </h1>
          <p className="text-gray-600 dark:text-gray-300">
            Hands-free voice conversation - just speak naturally
          </p>
        </div>

        {/* Error Display */}
        {error && (
          <div className="bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded mb-4">
            {error}
          </div>
        )}

        {/* Persona Selector */}
        {!session.isActive && (
          <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6 mb-6">
            <h2 className="text-lg font-semibold text-gray-800 dark:text-white mb-3">
              Select Mode
            </h2>
            <div className="flex gap-4">
              <button
                onClick={() => setPersona('adult')}
                className={`flex-1 py-4 px-6 rounded-lg font-semibold transition-all ${
                  persona === 'adult'
                    ? 'bg-gradient-to-br from-blue-500 to-indigo-600 text-white shadow-lg'
                    : 'bg-gray-100 dark:bg-gray-700 text-gray-700 dark:text-gray-300 hover:bg-gray-200 dark:hover:bg-gray-600'
                }`}
              >
                <div className="text-2xl mb-2">ðŸ‘¤</div>
                <div>Adult Mode</div>
              </button>
              <button
                onClick={() => setPersona('kid')}
                className={`flex-1 py-4 px-6 rounded-lg font-semibold transition-all ${
                  persona === 'kid'
                    ? 'bg-gradient-to-br from-pink-500 to-purple-600 text-white shadow-lg'
                    : 'bg-gray-100 dark:bg-gray-700 text-gray-700 dark:text-gray-300 hover:bg-gray-200 dark:hover:bg-gray-600'
                }`}
              >
                <div className="text-2xl mb-2">ðŸ§’</div>
                <div>Kid Mode</div>
              </button>
            </div>
          </div>
        )}

        {/* Main layout */}
        <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
          {/* Left: Voice orb + single button */}
          <div className="lg:col-span-1 flex flex-col gap-6">
            <div className="bg-white dark:bg-gray-800 rounded-2xl shadow-lg p-8 flex items-center justify-center relative min-h-[400px]">
              <VoiceOrb level={audioLevel} listening={session.isActive} size={320} className="pointer-events-none" />

              {!session.isActive ? (
                <button
                  onClick={startConversation}
                  className="absolute inset-0 m-auto w-48 h-48 rounded-full bg-gradient-to-br from-green-500 to-emerald-600 hover:from-green-600 hover:to-emerald-700 text-white font-bold text-xl shadow-2xl transition-all duration-300 focus:outline-none z-20 flex items-center justify-center border-4 border-white/30"
                  title="Start Conversation"
                  style={{ pointerEvents: 'auto' }}
                >
                  Start Conversation
                </button>
              ) : (
                <button
                  onClick={endConversation}
                  className="absolute inset-0 m-auto w-48 h-48 rounded-full bg-gradient-to-br from-red-500 to-rose-600 hover:from-red-600 hover:to-rose-700 text-white font-bold text-xl shadow-2xl transition-all duration-300 focus:outline-none z-20 flex items-center justify-center border-4 border-white/30 animate-[pulse_2s_ease-in-out_infinite]"
                  title="End Conversation"
                  style={{ pointerEvents: 'auto' }}
                >
                  End Conversation
                </button>
              )}

              {/* Status chip */}
              <div className="absolute top-4 right-4">
                <span className={`px-3 py-1 rounded-full text-xs font-semibold border ${
                  statusLabel() === 'Processing' ? 'bg-yellow-100 text-yellow-800 border-yellow-300 dark:bg-yellow-900/20 dark:text-yellow-300 dark:border-yellow-800' :
                  statusLabel() === 'AI Speaking' ? 'bg-blue-100 text-blue-800 border-blue-300 dark:bg-blue-900/20 dark:text-blue-300 dark:border-blue-800' :
                  statusLabel() === 'You Speaking' ? 'bg-emerald-100 text-emerald-800 border-emerald-300 dark:bg-emerald-900/20 dark:text-emerald-300 dark:border-emerald-800' :
                  statusLabel() === 'Listening' ? 'bg-purple-100 text-purple-800 border-purple-300 dark:bg-purple-900/20 dark:text-purple-300 dark:border-purple-800' :
                  'bg-gray-100 text-gray-800 border-gray-300 dark:bg-gray-700 dark:text-gray-200 dark:border-gray-600'
                }`}>
                  {statusLabel()}
                </span>
              </div>

              {session.isActive && (
                <div className="absolute bottom-4 text-gray-500 dark:text-gray-400 text-sm text-center">
                  Speak naturally - AI is listening
                </div>
              )}
            </div>
          </div>

          {/* Right: Conversation + Learning Progress */}
          <div className="lg:col-span-2 grid grid-cols-1 md:grid-cols-2 gap-6">
            {/* Conversation */}
            <div className="bg-white dark:bg-gray-800 rounded-2xl shadow-lg p-6">
              <h2 className="text-xl font-semibold text-gray-800 dark:text-white mb-4">Conversation</h2>
              <div className="space-y-3 max-h-96 overflow-y-auto">
                {conversationHistory.length === 0 ? (
                  <p className="text-gray-500 dark:text-gray-400 italic">
                    Start the conversation and speak naturally!
                  </p>
                ) : (
                  conversationHistory.map((msg, idx) => (
                    <div
                      key={idx}
                      className={`p-3 rounded-lg ${
                        msg.type === 'user'
                          ? 'bg-blue-100 dark:bg-blue-900 ml-8'
                          : 'bg-gray-100 dark:bg-gray-700 mr-8'
                      }`}
                    >
                      <div className="font-semibold text-sm mb-1">
                        {msg.type === 'user' ? 'You' : 'AI'}
                      </div>
                      <div className="text-gray-800 dark:text-gray-200">
                        {msg.text}
                      </div>
                    </div>
                  ))
                )}
              </div>
            </div>

            {/* Learning Progress */}
            <div className="bg-white dark:bg-gray-800 rounded-2xl shadow-lg p-6">
              <h2 className="text-xl font-semibold text-gray-800 dark:text-white mb-4">
                Learning Progress
              </h2>
              {learningProgress ? (
                <div className="space-y-4">
                  <div>
                    <div className="flex justify-between text-sm text-gray-600 dark:text-gray-400 mb-2">
                      <span>Goals Met</span>
                      <span>{learningProgress.goalsMet} / {learningProgress.totalGoals}</span>
                    </div>
                    <div className="w-full bg-gray-200 dark:bg-gray-700 rounded-full h-3">
                      <div
                        className="bg-green-600 h-3 rounded-full transition-all"
                        style={{
                          width: `${(learningProgress.goalsMet / learningProgress.totalGoals) * 100}%`
                        }}
                      />
                    </div>
                  </div>

                  {learningProgress.currentGoals && learningProgress.currentGoals.length > 0 && (
                    <div>
                      <h3 className="font-semibold text-gray-700 dark:text-gray-300 mb-2">
                        Current Learning Goals:
                      </h3>
                      <ul className="space-y-1">
                        {learningProgress.currentGoals.map((goal: string, idx: number) => (
                          <li key={idx} className="text-sm text-gray-600 dark:text-gray-400 flex items-start">
                            <span className="mr-2">â€¢</span>
                            <span>{goal}</span>
                          </li>
                        ))}
                      </ul>
                    </div>
                  )}
                </div>
              ) : (
                <p className="text-gray-500 dark:text-gray-400 italic">
                  No learning data yet. Start conversing to see AI learn about you!
                </p>
              )}
            </div>
          </div>
        </div>

        {/* Info Panel */}
        <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6 mt-6">
          <h2 className="text-xl font-semibold text-gray-800 dark:text-white mb-4">
            How It Works
          </h2>
          <div className="grid grid-cols-1 md:grid-cols-3 gap-4 text-sm">
            <div className="flex items-start gap-3">
              <span className="text-2xl">ðŸŽ¤</span>
              <div>
                <h3 className="font-semibold text-gray-800 dark:text-white">Hands-Free</h3>
                <p className="text-gray-600 dark:text-gray-400">Custom VAD automatically detects when you speak</p>
              </div>
            </div>
            <div className="flex items-start gap-3">
              <span className="text-2xl">ðŸ¤–</span>
              <div>
                <h3 className="font-semibold text-gray-800 dark:text-white">AI Response</h3>
                <p className="text-gray-600 dark:text-gray-400">ElevenLabs voice responds naturally</p>
              </div>
            </div>
            <div className="flex items-start gap-3">
              <span className="text-2xl">ðŸ§ </span>
              <div>
                <h3 className="font-semibold text-gray-800 dark:text-white">Learns About You</h3>
                <p className="text-gray-600 dark:text-gray-400">16 learning goals tracked automatically</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}
